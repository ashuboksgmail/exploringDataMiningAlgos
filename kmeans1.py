# -*- coding: utf-8 -*-
"""KMeans1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cYnodg8mo5LmtAZk7xAUA34qU2uWFNsw
"""



#mounting google driv
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#change directory 
# %cd /content/drive/MyDrive/DMV

#import necessary libraries
import numpy as np
import csv
import matplotlib.pyplot as plt

#read data from csv, 
#store data in numpy array called data
#skip the comlun name of data
data = np.genfromtxt('Household_Wealth.csv', delimiter=',', skip_header=1)

#calculate mean and standard deviation 
#z-score normalization
#store the data in new array scaled_data
mean = np.mean(data, axis=0)
std = np.std(data, axis=0)
scaled_data = (data - mean) / std

def k_means(data, k):
    # Initialize the centroids randomly
    centroids = data[np.random.choice(data.shape[0], k, replace=False)]
    
    # Loop until convergence
    for i in range(100):
        # Assign each data point to the closest centroid
        distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
        pred_y = np.argmin(distances, axis=0)
        
        # Update the centroids to the mean of the assigned data points
        for j in range(k):
            centroids[j] = np.mean(data[pred_y == j], axis=0)
        
        # Compute the within-cluster sum of squares
        wcss = np.sum((data - centroids[pred_y])**2)
        
        # Stop if the within-cluster sum of squares does not change much
        if i > 0 and np.abs(wcss - prev_wcss) < 1e-5:
            break
        
        prev_wcss = wcss
        
    return centroids, pred_y

#clustering using the K-mean function
#assign the returned values to centroids and pred_y
centroids, pred_y = k_means(scaled_data, 3)

#print the centroids
print("Centroid 1:", centroids[0])
print("Centroid 2:", centroids[1])
print("Centroid 3:", centroids[2])

#plot the K-Mean Clustering  
plt.scatter(scaled_data[:,0], scaled_data[:,1], c=pred_y)
plt.scatter(centroids[:,0], centroids[:,1], marker='x', s=200, linewidths=3, color='r')
plt.title('K-Means Clustering')
plt.xlabel('Wealth')
plt.ylabel('Income')
plt.show()

#plotting Elbow Method to determine best value of K
wcss_values = []
for k in range(1, 11):
    centroids, cluster_labels = k_means(scaled_data, k)
    distances = np.sqrt(((scaled_data - centroids[cluster_labels]) ** 2).sum(axis=1))
    wcss = (distances ** 2).sum()
    wcss_values.append(wcss)

plt.plot(range(1, 11), wcss_values)
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.title('Elbow Method')
plt.show()

#defining the silhoueet_score function 
def silhouette_score(data, pred_y):
    # Compute the distances between each point and all other points
    distances = np.sqrt(((data[:, np.newaxis] - data) ** 2).sum(axis=2))
    
    # Compute the average distance between each point and all other points in its cluster
    a = np.zeros(data.shape[0])
    for i in range(data.shape[0]):
        a[i] = np.mean(distances[i, pred_y == pred_y[i]])
    
    # Compute the average distance between each point and all points in the nearest cluster
    b = np.zeros(data.shape[0])
    for i in range(data.shape[0]):
        mask = pred_y != pred_y[i]
        if np.sum(mask) > 0:
            b[i] = np.min(distances[i, mask])
        else:
            b[i] = np.inf
    
    # Compute the silhouette score for each point
    s = (b - a) / np.maximum(a, b)
    
    # Compute the average silhouette score for all points
    return np.mean(s)

#Passing data and pred_y into the function to calculate and print score
score = silhouette_score(scaled_data, pred_y)
print("Silhouette score:", score)